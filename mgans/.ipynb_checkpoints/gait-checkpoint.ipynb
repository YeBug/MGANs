{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "from torch import from_numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "import os \n",
    "from scipy.misc import imresize\n",
    "import os.path as osp\n",
    "import h5py\n",
    "\n",
    "name = 'casia-px_ad_view-pei-62-nm'\n",
    "data_path = './data' # path to save .mat file\n",
    "label_list = list(np.array(range(124)) + 1)\n",
    "train_list = list(range(1,63)) # select the first 62 subjects for training\n",
    "test_list = list(range(63,125)) # select the last 62 subjects for testing\n",
    "cov_list = [1,2,3,4,5,6] # normal walking: 1-6, clothing variation: 7-8, carrying variation: 9-10 \n",
    "probe_cov = [5,6] # define probe set\n",
    "gallery_cov = [1,2,3,4] # define gallery set\n",
    "view_list = [1,2,3,4,5,6,7,8,9,10,11] # define viewing angles \n",
    "\n",
    "class Config:\n",
    "    batch_size = 128\n",
    "    view_dim = 11\n",
    "    num_class = 62\n",
    "    num_channel = 5\n",
    "    z_dim = 128\n",
    "    lrG = 5e-5\n",
    "    lrD = 5e-5\n",
    "    # img size\n",
    "    s1_1 = 64\n",
    "    s1_2 = 64\n",
    "\n",
    "    s2_1, s3_1, s4_1, s5_1 =\\\n",
    "        32, 16, 8, 4\n",
    "    s2_2, s3_2, s4_2, s5_2 =\\\n",
    "        32, 16, 8, 4\n",
    "\n",
    "    max_iter_step = 40000\n",
    "    lossD = []\n",
    "    lossPX = []\n",
    "    lossV = []\n",
    "    acc_list = []\n",
    "    \n",
    "f = h5py.File(osp.join(data_path,'pei_CASIA_5_3.mat'))\n",
    "data = f['data']\n",
    "label = f['label']\n",
    "view = f['view']\n",
    "cov = f['cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_samples(label, label_list, view, view_list, cov, cov_list):\n",
    "    idx = [ i for i in range(label.shape[0]) if label[i] in label_list and view[i] in view_list and cov[i] in cov_list]\n",
    "    return idx\n",
    "   \n",
    "opt=Config()\n",
    "\n",
    "data = np.transpose(data, [3,2,1,0])/255.0\n",
    "label = np.reshape(label,[-1]).astype(int)\n",
    "view = np.reshape(view,[-1]).astype(int)\n",
    "cov = np.reshape(cov,[-1]).astype(int)\n",
    "\n",
    "tmp = np.reshape(data,[-1,data.shape[1]*64*64])\n",
    "idx = np.isnan(np.sum(tmp,1)) + (np.sum(tmp,1)==0)\n",
    "data = data[idx==0,:,:,:]\n",
    "label = label[idx==0]\n",
    "cov = cov[idx==0]\n",
    "view = view[idx==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_num = opt.view_dim\n",
    "train_idx = filter_samples(label, train_list, cov, cov_list, view, view_list)\n",
    "test_idx = filter_samples(label, test_list, cov, cov_list, view, view_list)\n",
    "\n",
    "train_n = len(train_idx)\n",
    "test_n = len(test_idx)\n",
    "train_label = label[train_idx]\n",
    "test_label = label[test_idx]\n",
    "train_cov = cov[train_idx]\n",
    "test_cov = cov[test_idx]\n",
    "train_view = view[train_idx]\n",
    "test_view = view[test_idx]\n",
    "\n",
    "idxs = [ [train_idx[j] for j in range(train_n) if train_label[j] == train_label[i]] for i in range(train_n) ]\n",
    "index = []\n",
    "for (i,idx) in enumerate(idxs):\n",
    "    index.extend([(train_idx[i],j) for j in idx])\n",
    "index = np.array(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_enc = OneHotEncoder()\n",
    "label_enc.fit(np.reshape(train_label,[-1,1]))\n",
    "num_class = label_enc.transform(np.reshape(train_label,[-1,1])).toarray().shape[1]\n",
    "\n",
    "view_enc = OneHotEncoder()\n",
    "view_enc.fit(np.reshape(train_view,[-1,1]))\n",
    "\n",
    "channel_enc = OneHotEncoder()\n",
    "channel_enc.fit(np.reshape(range(opt.num_channel),[-1,1]))\n",
    "\n",
    "\n",
    "\n",
    "# If transform the latent reprensention from viewing angle 0 to viewing angle 36,\n",
    "# this function will return [[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]] when batch_size equals 1.\n",
    "def view_transform_encoder(v1,v2):\n",
    "    v1 = (v1-1).astype(int)\n",
    "    v2 = (v2-1).astype(int)\n",
    "    view_encode = np.zeros([v1.shape[0],opt.view_dim])\n",
    "    for i in range(v1.shape[0]):\n",
    "        if v1[i]>v2[i]:\n",
    "            if v1[i]-v2[i]>5:\n",
    "                view_encode[i,v1[i]:] = 1\n",
    "                view_encode[i,0:v2[i]] = 1\n",
    "            else:\n",
    "                view_encode[i,v2[i]:v1[i]] = -1\n",
    "        elif v1[i]<v2[i]:\n",
    "            if v2[i]-v1[i]>5:\n",
    "                view_encode[i,v2[i]:] = -1\n",
    "                view_encode[i,0:v1[i]] = -1\n",
    "            else:\n",
    "                view_encode[i,v1[i]:v2[i]] = 1\n",
    "    return autograd.Variable(from_numpy(view_encode).float().cuda())\n",
    "\n",
    "# Calculate rank-k accuracy\n",
    "def count_cmc_curve(label_list, num_rank):\n",
    "    acc = np.zeros([num_rank,opt.view_dim,opt.view_dim])\n",
    "    for i in range(1,opt.view_dim+1):\n",
    "        x1_idx = filter_samples(label, label_list, view, [i], cov, probe_cov)\n",
    "        x1 = np.zeros([len(x1_idx),opt.z_dim])\n",
    "        y1 = label[x1_idx]\n",
    "        v1 = np.zeros(y1.shape)\n",
    "        for k in range(0,x1.shape[0],opt.batch_size):\n",
    "            x = autograd.Variable(from_numpy(data[x1_idx[k:k+opt.batch_size],:,:,:]).float().cuda()) \n",
    "            tmp = netE(x)\n",
    "            x1[k:k+opt.batch_size,:] = tmp.data.cpu().numpy()\n",
    "            v1[k:k+opt.batch_size] = torch.max(F.softmax(netVes(tmp)),1)[1].data.cpu().numpy()+1 # Estimate the view information of probe set\n",
    "        \n",
    "        for j in range(1,opt.view_dim+1):    \n",
    "            x2_idx = filter_samples(label, label_list, view, [j], cov, gallery_cov)\n",
    "            x2 = np.zeros([len(x2_idx),opt.z_dim])\n",
    "            y2 = label[x2_idx]\n",
    "            v2 = np.zeros(y2.shape)\n",
    "            for k in range(0,x2.shape[0],opt.batch_size):\n",
    "                x = autograd.Variable(from_numpy(data[x2_idx[k:k+opt.batch_size],:,:,:]).float().cuda())\n",
    "                tmp = netE(x)\n",
    "                x2[k:k+opt.batch_size,:] = netE(x).data.cpu().numpy()\n",
    "                v2[k:k+opt.batch_size] = torch.max(F.softmax(netVes(tmp)),1)[1].data.cpu().numpy()+1 # Estimate the view information of gallery set\n",
    "            \n",
    "            # Transform the probe and gallery set to the same view\n",
    "            v = np.ones(y1.shape) * max(set(v2), key=list(v2).count)\n",
    "            for k in range(0,x1.shape[0],opt.batch_size):\n",
    "                x1[k:k+opt.batch_size,:] = netV(autograd.Variable(from_numpy(x1[k:k+opt.batch_size,:]).float().cuda()) ,view_transform_encoder(v1[k:k+opt.batch_size],v[k:k+opt.batch_size])).data.cpu().numpy()\n",
    "            v = np.ones(y2.shape) * max(set(v2), key=list(v2).count)\n",
    "            for k in range(0,x1.shape[0],opt.batch_size):\n",
    "                x2[k:k+opt.batch_size,:] = netV(autograd.Variable(from_numpy(x2[k:k+opt.batch_size,:]).float().cuda()) ,view_transform_encoder(v2[k:k+opt.batch_size],v[k:k+opt.batch_size])).data.cpu().numpy()\n",
    "            \n",
    "            # Normalize the latent representations\n",
    "            x1 = x1/np.tile(np.reshape(np.linalg.norm(x1,2,1),[-1,1]),[1,x1.shape[1]])\n",
    "            x2 = x2/np.tile(np.reshape(np.linalg.norm(x2,2,1),[-1,1]),[1,x2.shape[1]])\n",
    "            dist = distance.cdist(x1,x2)\n",
    "            \n",
    "            # Calculate the rank-k accuracies of all probe and gallery pairs\n",
    "            idx = np.argsort(dist,1)\n",
    "            match = np.zeros([idx.shape[0]])\n",
    "            for k in range(num_rank):\n",
    "                match += (y1==y2[idx[:,k]])\n",
    "                acc[k,i-1,j-1]=(np.average(match>0)*100)\n",
    "                \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.xavier_uniform(m.weight.data)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_uniform(m.weight.data)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(opt.z_dim+opt.num_channel, 128, 4, stride=2, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "        self.conv5 = nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,opt.z_dim+opt.num_channel, 1, 1)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = F.tanh(self.conv5(x))\n",
    "        x = x.view(-1,opt.s1_1,opt.s1_2)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, stride=2, padding=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, stride=2, padding=2, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.linear = nn.Linear(256*opt.s5_1*opt.s5_2, opt.z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,opt.s1_1,opt.s1_2)\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(-1,opt.num_channel,256*4*4)\n",
    "        x = torch.mean(x,1)\n",
    "        x = self.linear(x.view(-1,256*4*4))\n",
    "        return x\n",
    "\n",
    "class ViewTransformLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViewTransformLayer, self).__init__()\n",
    "        self.view_trans = nn.Linear(opt.view_dim,opt.z_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, view_encode):\n",
    "        z = x + self.view_trans(view_encode)\n",
    "        return z\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, stride=2, padding=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, stride=2, padding=2, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.linear1 = nn.Linear(256*opt.s5_1*opt.s5_2, opt.num_class+opt.num_channel+opt.view_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,opt.s1_1,opt.s1_2)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = x.view(-1,256*x.size(2)*x.size(3))\n",
    "        y = self.linear1(x)\n",
    "        return y\n",
    "\n",
    "class ViewEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViewEstimator, self).__init__()\n",
    "        self.linear1 = nn.Linear(opt.z_dim, opt.z_dim)\n",
    "        self.linear2 = nn.Linear(opt.z_dim,opt.view_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        y = self.linear2(x)\n",
    "        return y\n",
    "        \n",
    "\n",
    "with torch.cuda.device(0): \n",
    "    netE = Encoder().cuda().float()\n",
    "    netE.apply(weights_init)\n",
    "    netV = ViewTransformLayer().cuda().float()\n",
    "    netV.apply(weights_init)\n",
    "    netG = Generator().cuda().float()\n",
    "    netG.apply(weights_init)\n",
    "    netD = Discriminator().cuda().float()\n",
    "    netD.apply(weights_init)\n",
    "    netVes = ViewEstimator().cuda().float()\n",
    "    netVes.apply(weights_init)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augmentation(X):\n",
    "# [n,period,mx,my]\n",
    "    Y = np.ones(X.shape)*X[0,0,0,0]\n",
    "    Z = np.ones(X.shape)\n",
    "    n = X.shape[0]\n",
    "    for i in range(n):\n",
    "        for t in range(opt.num_channel):\n",
    "            shift = np.random.randint(-8,8)\n",
    "            if shift>0:\n",
    "                Y[i,t,:,shift:64] = X[i,t,:,0:64-shift]\n",
    "            else:\n",
    "                Y[i,t,:,0:64+shift] = X[i,t,:,0-shift:64]     \n",
    "            k1= np.random.randint(0,int(64*0.2))\n",
    "            b = imresize(Y[i,t,:,:],[64-k1,64-k1])\n",
    "            k2 = np.random.randint(0,k1+1)\n",
    "            Z[i,t,:,:] = Z[i,t,:,:]*b[0,0]\n",
    "            Z[i,t,k2:k2+64-k1,k2:k2+64-k1] = b\n",
    "    return Z/255.0\n",
    "\n",
    "def construct_train(opt):\n",
    "    idx_same = np.random.randint(0, index.shape[0], [opt.batch_size])\n",
    "    idx1 = index[idx_same,0]\n",
    "    idx2 = index[idx_same,1]\n",
    "    \n",
    "    y = np.reshape(label[idx2],[-1,1])\n",
    "    enc_y = label_enc.transform(y).toarray()\n",
    "    x1 = data[idx1,:,:,:]\n",
    "    x2 = data[idx2,:,:,:]\n",
    "    z_channel = np.random.randint(0,opt.num_channel,opt.batch_size)\n",
    "    x2 = x2[range(opt.batch_size),z_channel,:,:]\n",
    "    z_channel = np.reshape(z_channel,[-1,1])\n",
    "    enc_z_channel = channel_enc.transform(z_channel).toarray()\n",
    "    \n",
    "    view1 = view[idx1]\n",
    "    view2 = view[idx2]\n",
    "    enc_view2 = view_enc.transform(np.reshape(view2,[-1,1])).toarray()\n",
    "    \n",
    "    x1 = augmentation(x1)\n",
    "    x1 = autograd.Variable(from_numpy(x1).float().cuda())\n",
    "    x2 = autograd.Variable(from_numpy(x2).float().cuda())\n",
    "    y = autograd.Variable(from_numpy(y).float().cuda())\n",
    "    enc_view_trans = view_transform_encoder(view1,view2)\n",
    "    view1 = autograd.Variable(from_numpy(view1-1).long().cuda())\n",
    "    enc_z_channel = autograd.Variable(from_numpy(enc_z_channel).float().cuda())\n",
    "    enc_y = autograd.Variable(from_numpy(enc_y).float().cuda())\n",
    "    enc_view2 = autograd.Variable(from_numpy(enc_view2).float().cuda())\n",
    "\n",
    "    return x1, x2, view1, enc_view_trans, enc_z_channel, enc_y, enc_view2\n",
    "\n",
    "optG=optim.RMSprop([{'params':netE.parameters()},{'params':netV.parameters()},{'params':netG.parameters()}],lr=5e-5,weight_decay=1.5e-4) \n",
    "optD=optim.RMSprop(netD.parameters(),lr=5e-5, weight_decay=1.5e-4)\n",
    "optVes=optim.Adam([{'params':netVes.parameters()}],lr=1e-4,weight_decay=1.5e-4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the encoder, generator, discriminator and the view transform layer.\n",
    "\n",
    "global_it = 0\n",
    "with torch.cuda.device(0):\n",
    "    for it in range(opt.max_iter_step):\n",
    "        \n",
    "        global_it += 1\n",
    "        print(global_it)\n",
    "        \n",
    "        for _ in range(5):  \n",
    "            netD.zero_grad()\n",
    "            netVes.zero_grad()\n",
    "            \n",
    "            # Sampling batchsize pairs with the same identity\n",
    "            x1, x2, view1, enc_view_trans, enc_z_channel, enc_y, enc_view2 = construct_train(opt)\n",
    "            \n",
    "            # Encoder gait templates x1 to latent reprensentations z_hidden by view information \"enc_view_trans\"\n",
    "            z_hidden = netV(netE(x1), enc_view_trans)  \n",
    "            noise = np.random.rand(opt.batch_size,opt.z_dim)*1e-4\n",
    "            z_hidden = z_hidden + autograd.Variable(from_numpy(noise).float().cuda())\n",
    "    \n",
    "            # Concat one hot representations of channel to z_hidden\n",
    "            z = torch.cat((z_hidden,enc_z_channel), 1) \n",
    "            \n",
    "            (x2D) = netD(x2) \n",
    "            fake_x1 = netG(z)\n",
    "            (x1D) = netD(fake_x1)\n",
    "            \n",
    "            # flag is one-hot encoding vector, represented by concatenating the one-hot representation of view, channel and the identity of samples \n",
    "            flag = torch.cat((enc_view2,enc_y,enc_z_channel),1)\n",
    "            lossD = torch.mean(torch.sum((-x2D+x1D)*flag,1)) \n",
    "\n",
    "            # Define gradient penalty loss of WGANs\n",
    "            alpha = torch.rand(opt.batch_size, 1, 1)\n",
    "            alpha = alpha.expand(x2.size())\n",
    "            alpha = autograd.Variable(alpha.float().cuda())\n",
    "            interpolates = alpha*x2 + ((1-alpha)*fake_x1)\n",
    "            disc_interpolates = torch.mean(torch.sum(netD(interpolates)*flag,1))\n",
    "            gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones(disc_interpolates.size()).float().cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "            gradients = gradients.view(-1,64*64)\n",
    "            gradients_penalty = ((gradients.norm(2, dim=1)-1)**2).mean() * 10\n",
    "\n",
    "            lossD2 = lossD + gradients_penalty  # Discriminator's adversarial loss               \n",
    "            lossD2.backward()\n",
    "            optD.step()\n",
    "\n",
    "        for _ in range(1):\n",
    "            netE.zero_grad()\n",
    "            netV.zero_grad()\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            x1, x2, view1, enc_view_trans, enc_z_channel, enc_y, enc_view2  = construct_train(opt)\n",
    "            z_hidden = netE(x1)        \n",
    "            \n",
    "            noise = np.random.rand(opt.batch_size,opt.z_dim)*1e-4          \n",
    "            z = torch.cat((netV(z_hidden,enc_view_trans) + autograd.Variable(from_numpy(noise).float().cuda()),enc_z_channel), 1)\n",
    "            fake_x1 = netG(z)\n",
    "            (x1D) = netD(fake_x1) \n",
    "            flag = torch.cat((enc_view2,enc_y,enc_z_channel),1)\n",
    "\n",
    "            lossG1 = torch.mean(torch.abs(x2-fake_x1)) # Pixel-wise loss\n",
    "            lossG2 = torch.mean(torch.sum(-x1D*flag,1)) # Generator's adversarial loss\n",
    "            lossG = lossG1 + 1e-5*lossG2\n",
    "            \n",
    "            lossG.backward(retain_graph=True)\n",
    "            optG.step()\n",
    "            \n",
    "        if global_it%1000==0:\n",
    "            print('iter %d:'%(global_it))\n",
    "            torch.save(netE.state_dict(), osp.join('./model','{}-E.ptm'.format(name)))\n",
    "            torch.save(netV.state_dict(), osp.join('./model','{}-V.ptm'.format(name)))\n",
    "            torch.save(netG.state_dict(), osp.join('./model','{}-G.ptm'.format(name)))\n",
    "            torch.save(netD.state_dict(), osp.join('./model','{}-D.ptm'.format(name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the view-angle classifier\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    for it in range(10000):\n",
    "        global_it += 1\n",
    "        for _ in range(1):\n",
    "            netVes.zero_grad()\n",
    "            x1, x2, view1, enc_view_trans, enc_z_channel, enc_y, enc_view2  = construct_train(opt)\n",
    "            z_hidden = netE(x1)        \n",
    "            lossFun = nn.CrossEntropyLoss()\n",
    "            lossG3 = lossFun(netVes(z_hidden), view1) \n",
    "\n",
    "            lossG3.backward()\n",
    "            optVes.step()\n",
    "            \n",
    "        if global_it%1000==0:\n",
    "            torch.save(netVes.state_dict(), osp.join('./model','{}-Ves.ptm'.format(name)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
